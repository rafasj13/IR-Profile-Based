{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3361028f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49254,
     "status": "ok",
     "timestamp": 1680948858605,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "3361028f",
    "outputId": "75f072a0-ed6e-4eb2-9439-8b12370e2c81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (1.24.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from matplotlib) (1.24.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nltk in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: click in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sklearn in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (0.0.post1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langdetect in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\rafas\\anaconda3\\envs\\ir-test\\lib\\site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Only for installations\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "080a2e63",
   "metadata": {
    "id": "080a2e63"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer, WordNetLemmatizer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string as st\n",
    "from langdetect import detect, detect_langs\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c9a05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50185,
     "status": "ok",
     "timestamp": 1680949064555,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "807c9a05",
    "outputId": "b8e1614a-a653-4e9d-d7f0-59d75a5eb505"
   },
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf80db6",
   "metadata": {
    "id": "0bf80db6"
   },
   "source": [
    "## We define here the functions we are going to apply to the dataframes to preprocess the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e0920",
   "metadata": {
    "id": "487e0920"
   },
   "source": [
    "### Function that detects the language of a document by its title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ecb109",
   "metadata": {
    "id": "63ecb109"
   },
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'no'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e80063",
   "metadata": {
    "id": "65e80063"
   },
   "source": [
    "### Functions to remove punctuations and non alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37bf8d",
   "metadata": {
    "id": "bf37bf8d"
   },
   "outputs": [],
   "source": [
    "# Remove all punctuations from the text\n",
    "\n",
    "def remove_punct(text):\n",
    "    return (\"\".join([ch for ch in text if ch not in st.punctuation]))\n",
    "\n",
    "\n",
    "def remove_no_alphanumeric(text):\n",
    "    return (\"\".join([re.sub(r'\\W+', ' ', ch) for ch in text]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8aeed3",
   "metadata": {
    "id": "dc8aeed3"
   },
   "source": [
    "### Function to split the text by words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749edc34",
   "metadata": {
    "id": "749edc34"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.split('\\s+' ,text)\n",
    "    return [x.lower() for x in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366f5d4",
   "metadata": {
    "id": "c366f5d4"
   },
   "source": [
    "### Function to remove words with less than 3 characters from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7e53f",
   "metadata": {
    "id": "57b7e53f"
   },
   "outputs": [],
   "source": [
    "def remove_small_words(text):\n",
    "    return [x for x in text if len(x) > 3 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba14cfa",
   "metadata": {
    "id": "9ba14cfa"
   },
   "source": [
    "### Function to remove all english stopwords from the text that do not give us information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rxKq4VCQwuOc",
   "metadata": {
    "id": "rxKq4VCQwuOc"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce87e8",
   "metadata": {
    "id": "02ce87e8"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in nltk.corpus.stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fabe9c",
   "metadata": {
    "id": "e6fabe9c"
   },
   "source": [
    "### Lemmatization of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ff223",
   "metadata": {
    "id": "649ff223"
   },
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    word_net = WordNetLemmatizer()\n",
    "    return [word_net.lemmatize(word) for word in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jSULkcyvtEiV",
   "metadata": {
    "id": "jSULkcyvtEiV"
   },
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678141b2",
   "metadata": {
    "id": "678141b2"
   },
   "outputs": [],
   "source": [
    "def return_sentences(tokens):\n",
    "    return \" \".join([word for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257561c",
   "metadata": {
    "id": "a257561c"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    res = remove_punct(text)\n",
    "    res = remove_no_alphanumeric(res)\n",
    "    res = tokenize(res)\n",
    "    res = remove_small_words(res)\n",
    "    res = remove_stopwords(res)\n",
    "    res = lemmatize(res)\n",
    "    res = return_sentences(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1acec0",
   "metadata": {
    "id": "5b1acec0"
   },
   "source": [
    "## We read the 3 files of the dataset, the content of these files is:\n",
    "- **metadata.csv**: Contains the different documents of the dataset with some information such as the title or abstract\n",
    "- **topics-rnd3.csv**: Contains the different 40 topics of the dataset, which are going to be used as profiles for the IR system\n",
    "- **qrels.csv**: Contains the relevent judgements for several documents for all the queries/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f50be3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5125,
     "status": "ok",
     "timestamp": 1680949254427,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "89f50be3",
    "outputId": "88a91bbe-122d-4d42-aecc-1e9127b01cd2"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"metadata.csv\")\n",
    "judgements = pd.read_csv(\"qrels.csv\")\n",
    "topics = pd.read_csv(\"topics-rnd3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c03e60",
   "metadata": {
    "id": "f4c03e60",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d72f3",
   "metadata": {
    "id": "6b1d72f3"
   },
   "source": [
    "## We delete all the unecessary columns of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa4113",
   "metadata": {
    "id": "39aa4113"
   },
   "outputs": [],
   "source": [
    "data = data.drop(['sha', 'source_x', 'doi', 'pmcid', 'pubmed_id', 'license', 'publish_time', 'authors', \n",
    "                  'journal', 'mag_id','who_covidence_id', 'arxiv_id', 'pdf_json_files', 'pmc_json_files',\n",
    "                   'url', 's2_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ecc15",
   "metadata": {
    "id": "280ecc15"
   },
   "source": [
    "## All missing and unkown values were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340531d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1680949259593,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "b340531d",
    "outputId": "580f43c9-50df-4084-8fd3-f3115841cc5f"
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c1ca5",
   "metadata": {
    "id": "032c1ca5"
   },
   "outputs": [],
   "source": [
    "data.replace('Unknown', np.nan, inplace=True)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b40b2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1680949261413,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "95b40b2d",
    "outputId": "d24513f9-3d21-4930-eef9-47d6cc54849b"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4030fab1",
   "metadata": {
    "id": "4030fab1"
   },
   "source": [
    "## Since the dataset is really big, we decided to make a sample only choosing those documents that have available relevance judgements for the first 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437bc56e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1680949265908,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "437bc56e",
    "outputId": "7f751144-6e50-4af0-e5eb-13a0624ebbd0"
   },
   "outputs": [],
   "source": [
    "topics = topics[topics['topic-id'] <= 10]\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e437d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1680949266213,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "742e437d",
    "outputId": "d531af66-c4a2-4e65-ae44-83eeb41a2c08"
   },
   "outputs": [],
   "source": [
    "judgements = judgements[judgements['topic-id'] <= 10]\n",
    "cods = judgements['cord-id'].tolist()\n",
    "judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0562880",
   "metadata": {
    "id": "d0562880"
   },
   "outputs": [],
   "source": [
    "data = data[data['cord_uid'].isin(cods)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b6ae57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1680949269475,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "b6b6ae57",
    "outputId": "ae760f39-44a2-418c-cc8f-f35fdcf64940"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d7e351",
   "metadata": {
    "id": "37d7e351"
   },
   "source": [
    "## Afterwards, all text that are not written in english are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a74be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "executionInfo": {
     "elapsed": 34006,
     "status": "ok",
     "timestamp": 1680949305961,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "f78a74be",
    "outputId": "43867437-e3bc-4958-a191-f5b940860b9c"
   },
   "outputs": [],
   "source": [
    "data['lan'] = data['title'].apply(lambda x: detect_language(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d7e47",
   "metadata": {
    "id": "f47d7e47"
   },
   "outputs": [],
   "source": [
    "english_docs = data[data['lan'] == 'en']['cord_uid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46fcf9",
   "metadata": {
    "id": "cf46fcf9"
   },
   "outputs": [],
   "source": [
    "data = data[data['lan'] == 'en']\n",
    "judgements = judgements[judgements['cord-id'].isin(english_docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736bf0dd",
   "metadata": {
    "id": "736bf0dd"
   },
   "source": [
    "## We apply all the preprocessing steps defined above to the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05f20e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91611,
     "status": "ok",
     "timestamp": 1680949397561,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "ba05f20e",
    "outputId": "7edf223e-93da-4610-a65a-b10a6e29d9c2"
   },
   "outputs": [],
   "source": [
    "data['clean_text'] = data['abstract'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6332a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1680949397562,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "0c6332a0",
    "outputId": "50509d06-cf69-4d6e-d17b-e5e583169818"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b9feb",
   "metadata": {
    "id": "ed7b9feb"
   },
   "source": [
    "## We apply the same preprocessing to the different queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809e417",
   "metadata": {
    "id": "2809e417"
   },
   "outputs": [],
   "source": [
    "topics = topics[['query']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8aa8c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1680949397564,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "fc8aa8c4",
    "outputId": "05ea191b-b2ec-4351-99ad-875090d88c31"
   },
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c0992c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1680949397565,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "f5c0992c",
    "outputId": "b3a69465-0196-43cc-9f3a-f82614f724d6"
   },
   "outputs": [],
   "source": [
    "topics['clean_query'] = topics['query'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b632bc16",
   "metadata": {
    "id": "b632bc16"
   },
   "source": [
    "## The queries are expanded using WordNet Thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320275d8",
   "metadata": {
    "id": "320275d8"
   },
   "outputs": [],
   "source": [
    "def get_synonims(text):\n",
    "    res = []\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        res.append(w)\n",
    "        for syn in wordnet.synsets(w):\n",
    "            for lem in syn.lemmas():\n",
    "                if lem.name().find(\"_\") < 0 and lem.name() not in res:\n",
    "                    res.append(lem.name().lower())\n",
    "    return return_sentences(res)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ccc6af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1680949397566,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "04ccc6af",
    "outputId": "2254ee5d-b1d7-4e9b-e040-2e5ff03a60b6"
   },
   "outputs": [],
   "source": [
    "topics['synonims'] = topics['clean_query'].apply(lambda x: get_synonims(x))\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186278f9",
   "metadata": {
    "id": "186278f9"
   },
   "source": [
    "## The vocabulary present in both, the documents and queries is collected so it is used in both TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712455b5",
   "metadata": {
    "id": "712455b5"
   },
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "doc_text = data['clean_text'].apply(tokenize).tolist()\n",
    "for tokens in doc_text:\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "            \n",
    "doc_text = topics['synonims'].apply(tokenize).tolist()\n",
    "for tokens in doc_text:\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bc494c",
   "metadata": {
    "id": "79bc494c"
   },
   "source": [
    "## We calculate the TF-IDF for both, the documents and queries (in the case of queries idf component is not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2d49c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1680949433381,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "05c2d49c",
    "outputId": "8a4a1f56-981d-43e6-92a0-c8bb2911eecb"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(vocabulary = vocabulary)\n",
    "tfidf_vect = tfidf.fit_transform(data['clean_text'])\n",
    "tfidf_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f732c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1680949433384,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "090f732c",
    "outputId": "3605bf79-bd5f-42d6-bb0d-3269f1f6fb00"
   },
   "outputs": [],
   "source": [
    "tfidf_queries = TfidfVectorizer(vocabulary = vocabulary, use_idf = False)\n",
    "tfidf_vect_queries = tfidf.fit_transform(topics['synonims'])\n",
    "tfidf_vect_queries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d07197",
   "metadata": {
    "id": "b9d07197"
   },
   "source": [
    "## Using the TF-IDF, the cosine similarities among documents and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c523a",
   "metadata": {
    "id": "af0c523a"
   },
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(tfidf_vect, tfidf_vect_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1676536",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1680949433386,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "b1676536",
    "outputId": "41f4202b-9e1c-4bf3-b88f-318f3502b0db",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "queries = topics['query'].tolist()\n",
    "similarity_df = pd.DataFrame(similarity_matrix, columns = queries)\n",
    "similarity_df.insert(loc=0, column = 'doc_id', value=data['cord_uid'].tolist())\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef50a0d",
   "metadata": {
    "id": "0ef50a0d"
   },
   "source": [
    "## A matrix with the relevance judgements for every document and query pair necessary for evaluation is generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485aaa9",
   "metadata": {
    "id": "4485aaa9"
   },
   "outputs": [],
   "source": [
    "real_values = data[['cord_uid', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1ff9f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 42706,
     "status": "ok",
     "timestamp": 1680951469227,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "0ea1ff9f",
    "outputId": "27190f98-6b38-4eb3-b11c-dca45e8c26f2"
   },
   "outputs": [],
   "source": [
    "queries = topics['query'].tolist()\n",
    "for i in range(len(queries)):\n",
    "    col = []\n",
    "    for j in real_values.index:\n",
    "        cord_id = real_values['cord_uid'][j]\n",
    "        jud_list = judgements.loc[(judgements['topic-id'] == i+1) & (judgements['cord-id'] == cord_id), 'judgement'].tolist()\n",
    "        if len(jud_list) > 0:\n",
    "            col.append(jud_list[0])\n",
    "        else:\n",
    "            col.append(-1)\n",
    "    real_values.insert(loc = i+2, column = queries[i], value=col)\n",
    "real_values = real_values.replace(2,1)\n",
    "real_values.drop(['title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa6847",
   "metadata": {
    "id": "48fa6847"
   },
   "source": [
    "## Evaluation of the IR system with the different queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ecb2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ex,Q,R):\n",
    "    nq=len(Q)\n",
    "    nd=len(Q[0])\n",
    "    R_=np.array(R)\n",
    "    R_=.5*(R_+1)\n",
    "    Prec_tot=[]\n",
    "    Rec_tot=[]\n",
    "    \n",
    "    def compute_PR():\n",
    "        Prec_tot=[]\n",
    "        Rec_tot=[]        \n",
    "       \n",
    "        for q in range(nq):\n",
    "            q1=q+1\n",
    "            r=R_[q,:]\n",
    "            Prec_q=[]\n",
    "            Rec_q=[]\n",
    "            for k in range(nd):\n",
    "                k1=k+1\n",
    "                Prec=np.sum(r[:k1])/k1\n",
    "                Rec=np.sum(r[:k1])/np.sum(r)                \n",
    "    \n",
    "                Prec_q.append(Prec)\n",
    "                Rec_q.append(Rec)\n",
    "            Prec_tot.append(Prec_q)\n",
    "            Rec_tot.append(Rec_q)\n",
    "        Prec_tot=np.array(Prec_tot)\n",
    "        Rec_tot=np.array(Rec_tot)\n",
    "        return Prec_tot, Rec_tot\n",
    "    \n",
    "    \n",
    "    def compute_TPFP(TP_rate=None):\n",
    "        TP_tot=[]        \n",
    "        FP_tot=[]        \n",
    "        for q in range(nq):\n",
    "            q1=q+1\n",
    "            r=R_[q,:]\n",
    "            nr=1-r\n",
    "            TP_q=[]\n",
    "            FP_q=[]\n",
    "            for k in range(nd):\n",
    "                k1=k+1\n",
    "                TP=np.sum(r[:k1])/np.sum(r)                \n",
    "                FP=np.sum(nr[:k1])/np.sum(nr)\n",
    "                \n",
    "\n",
    "                TP_q.append(TP)\n",
    "                FP_q.append(FP)\n",
    "            TP_tot.append(TP_q)\n",
    "            FP_tot.append(FP_q)\n",
    "        TP_tot=np.array(TP_tot)\n",
    "        FP_tot=np.array(FP_tot)\n",
    "        return TP_tot, FP_tot        \n",
    "\n",
    "    x_axis, y_axis = [],[]\n",
    "    \n",
    "    if ex=='prec_rec' or ex=='all':        \n",
    "        Prec_tot, Rec_tot=compute_PR()\n",
    " \n",
    "        for q in range(nq):\n",
    "            q1=q+1\n",
    "     \n",
    "            Rec_q=Rec_tot[q,:]\n",
    "            Prec_q=Prec_tot[q,:]\n",
    "\n",
    "            x_axis.append(np.array(Rec_q))\n",
    "            y_axis.append(np.array(Prec_q))\n",
    "\n",
    "    if ex=='r-prec' or ex=='all':        \n",
    "        if len(Prec_tot):\n",
    "            Prec_tot, Rec_tot=compute_PR()\n",
    "\n",
    "        for q in range(nq):            \n",
    "            Rec_q=Rec_tot[q,:]\n",
    "            Prec_q=Prec_tot[q,:]\n",
    "            r=int(np.sum(R_[q]))\n",
    "            q1=q+1\n",
    "            print('AP=%.2f'%(Prec_q[r-1]))\n",
    "            \n",
    "    if ex=='map' or ex=='all':        \n",
    "        if len(Prec_tot):\n",
    "            Prec_tot, Rec_tot=compute_PR()\n",
    "        APs=[]\n",
    "        for q in range(nq):            \n",
    "            Prec_q=Prec_tot[q,:]            \n",
    "            r=int(np.sum(R_[q]))\n",
    "            q1=q+1\n",
    "\n",
    "            rs=np.where(R_[q]==1)[0]+1\n",
    "            AP=np.mean(Prec_q[np.where(R_[q]==1)])            \n",
    "            APs.append(AP)        \n",
    "        print('MAP=%.2f'%(np.mean(np.array(APs))))\n",
    "        \n",
    "        \n",
    "    if ex=='roc' or ex=='all' or ex=='auc':\n",
    "        TP_tot, FP_tot=compute_TPFP()    \n",
    "        for q in range(nq):\n",
    "            q1=q+1\n",
    "   \n",
    "            TP_q=TP_tot[q,:]\n",
    "            FP_q=FP_tot[q,:]\n",
    "            TP_q_=np.hstack([0,TP_q,1])\n",
    "            FP_q_=np.hstack([0,FP_q,1])\n",
    "\n",
    "    \n",
    "            \n",
    "            x_axis.append(np.array(FP_q_))\n",
    "            y_axis.append(np.array(TP_q_))\n",
    "            \n",
    "            if ex=='auc' or ex=='all':\n",
    "                AUC=[]\n",
    "                for i_x in range(TP_q_.size-1):\n",
    "                    delta_x=FP_q_[i_x+1]-FP_q_[i_x]\n",
    "                    base=TP_q_[i_x+1]+TP_q_[i_x]\n",
    "                    AUC.append(base*delta_x/2)\n",
    "                AUC=np.array(AUC)\n",
    "                AUC=AUC[AUC>0]\n",
    "                print('AUC = %.2f\\n\\n\\n\\n'% np.sum(AUC))      \n",
    "            \n",
    "    if ex=='clear':\n",
    "        return\n",
    "    \n",
    "    return x_axis, y_axis\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d46c5ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3845,
     "status": "ok",
     "timestamp": 1680962307441,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "5d46c5ba",
    "outputId": "d7a3b1e3-5f59-4138-b232-0978216e8e42",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig_roc, ax_roc = plt.subplots(figsize=(16, 8))\n",
    "fig_pr, ax_pr = plt.subplots(figsize=(16, 8))\n",
    "for i in range(len(queries)):\n",
    "    print(\"Query \" + str(i) + \": \" + queries[i])\n",
    "    \n",
    "    # Steps:\n",
    "    # 1. Selection of the query\n",
    "    # 2. Selection of the non-negatives\n",
    "    # 3. Merge cosine distances and relevance judgements\n",
    "    r_i = real_values[real_values[queries[i]] > -1].rename(columns={queries[i]: 'judgement'})\n",
    "    q_i = similarity_df[similarity_df['doc_id'].isin(r_i['cord_uid'].tolist())].rename(columns = {queries[i]: 'query'})\n",
    "    r_i = r_i.set_index('cord_uid')[['judgement']]\n",
    "    q_i = q_i.set_index('doc_id')[['query']]\n",
    "    rq_i = pd.merge(q_i, r_i, how='left',left_index=True, right_index=True)\n",
    "    # 4. Sort according to distance\n",
    "    rq_i = rq_i.sort_values(by='query', ascending = False)\n",
    "    # 5. Separate and evaluate\n",
    "    q_i = rq_i[['query']]\n",
    "    r_i = rq_i[['judgement']]\n",
    "    r_i_np = r_i.T.to_numpy()\n",
    "    q_i_np = q_i.T.to_numpy()\n",
    "    \n",
    "    eval = 'all'\n",
    "    x_axis, y_axis = evaluate(eval, q_i_np, r_i_np)\n",
    "\n",
    "         \n",
    "    if eval == 'prec_rec' or eval == 'all':  \n",
    "        ax_pr.plot(x_axis[0], y_axis[0])\n",
    "\n",
    "    if eval == 'auc' or eval == 'roc' or eval == 'all':\n",
    "        if eval == 'all':\n",
    "            ax_roc.plot(x_axis[1], y_axis[1])\n",
    "        else: \n",
    "            ax_roc.plot(x_axis[0], y_axis[0])\n",
    "\n",
    "ax_pr.set_xlabel('Recall')\n",
    "ax_pr.set_ylabel('Precision')\n",
    "ax_pr.legend([f'{queries[i]}' for i in range(len(queries))])\n",
    "fig_pr.show()\n",
    "\n",
    "\n",
    "ax_roc.set_xlabel('FPR')\n",
    "ax_roc.set_ylabel('TPR')\n",
    "ax_roc.legend([f'{queries[i]}' for i in range(len(queries))])\n",
    "fig_roc.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfcf2b3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ef2a7",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0a3d8",
   "metadata": {},
   "source": [
    "## Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1db884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moviesdf = pd.read_csv('wiki-movies.csv')\n",
    "moviesdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3abd27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#moviesdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65436d7d",
   "metadata": {},
   "source": [
    "### Check Categories "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30485481",
   "metadata": {},
   "source": [
    "- There are some repeated links and unknowns. Therefore, we will replace the unknowns by Nans and remove the links.\n",
    "- There are films from many Ethnicities. We will focus on the English-speaking ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134214de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for col in moviesdf.columns:\n",
    "#    print(f'=== {col} ===')\n",
    "#    print(moviesdf[col].value_counts(),'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesdf = moviesdf[moviesdf['Origin/Ethnicity'].isin(['American','British','Canadian','Australian'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e65b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesdf['Director'] = moviesdf['Director'].apply(lambda x: x.lower())\n",
    "moviesdf.replace('unknown', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3faede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesdf = moviesdf.drop_duplicates(subset=['Wiki Page'])\n",
    "moviesdf = moviesdf.drop_duplicates(subset=['Plot'])\n",
    "#moviesdf['Wiki Page'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96888b36",
   "metadata": {},
   "source": [
    "### Select and balance Genre categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf862ea1",
   "metadata": {},
   "source": [
    "- Simplifico quedandome con las top categorias mas frequentes > 500\n",
    "- Balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db1526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moviesdf['Genre'].value_counts()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topGenres = moviesdf['Genre'].value_counts()[:8].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "drama_comedy = moviesdf[moviesdf['Genre'].isin(['drama','comedy'])]\n",
    "rest = moviesdf[moviesdf['Genre'].isin(topGenres.drop(['drama','comedy']))]\n",
    " \n",
    "# Downsample mayority class\n",
    "downsampled = resample(drama_comedy, \n",
    "                        replace=False,    \n",
    "                        n_samples=2000,    \n",
    "                        random_state=123)   \n",
    "\n",
    "moviesTOP = pd.concat([downsampled, rest],axis=0)\n",
    "moviesTOP['Genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb514f7",
   "metadata": {},
   "source": [
    "### Column concatenate\n",
    "- We will concatenate the columns to have  common text, where we will drop the nans if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7172db",
   "metadata": {},
   "outputs": [],
   "source": [
    "namecol = moviesTOP.columns.drop(['Origin/Ethnicity','Release Year','Wiki Page'])\n",
    "moviesTOP['Message'] = moviesTOP[namecol].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1f73d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "moviesTOP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62af40f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = moviesTOP[['Message','Genre']].copy()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc2016",
   "metadata": {},
   "source": [
    "- Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_jud = {\"preferences\": [[\" drama\", \"thriller\"],\n",
    "                         [\" horror\", \"adventure\"],\n",
    "                         [\" comedy\", \"crime\"],\n",
    "                         [\" western\", \"action\"],\n",
    "                         [\" adventure\", \"crime\"]]}\n",
    "relevance_jud = pd.DataFrame(relevance_jud)\n",
    "relevance_jud['preferences'] = relevance_jud['preferences'].apply(lambda x: \", \".join(x))\n",
    "relevance_jud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e330a",
   "metadata": {},
   "source": [
    "## Text Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14381c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['Message'].apply(lambda x: clean_text(x))\n",
    "[['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0791a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_jud['clean_query'] = relevance_jud['preferences'].apply(lambda x: clean_text(x))\n",
    "relevance_jud['synonims'] = relevance_jud['clean_query'].apply(lambda x: get_synonims(x))\n",
    "relevance_jud[['synonims']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc09d2",
   "metadata": {},
   "source": [
    "- Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ee8a6",
   "metadata": {
    "id": "712455b5"
   },
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "doc_text = data['clean_text'].apply(tokenize).tolist()\n",
    "for tokens in doc_text:\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d0b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text = relevance_jud['synonims'].apply(tokenize).tolist()\n",
    "for tokens in doc_text:\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691bdbb",
   "metadata": {
    "id": "79bc494c"
   },
   "source": [
    "* TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4e403",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1680949433381,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "05c2d49c",
    "outputId": "8a4a1f56-981d-43e6-92a0-c8bb2911eecb"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(vocabulary = vocabulary)\n",
    "tfidf_vect = tfidf.fit_transform(data['clean_text'])\n",
    "tfidf_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e9435",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1680949433384,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "090f732c",
    "outputId": "3605bf79-bd5f-42d6-bb0d-3269f1f6fb00"
   },
   "outputs": [],
   "source": [
    "tfidf_queries = TfidfVectorizer(vocabulary = vocabulary, use_idf = False)\n",
    "tfidf_vect_queries = tfidf.fit_transform(relevance_jud['synonims'])\n",
    "tfidf_vect_queries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd22db",
   "metadata": {
    "id": "b9d07197"
   },
   "source": [
    "- Cosine similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8719180f",
   "metadata": {
    "id": "af0c523a"
   },
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(tfidf_vect, tfidf_vect_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560bda05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1680949433386,
     "user": {
      "displayName": "Julio Martinez Bastida",
      "userId": "12339153332611120329"
     },
     "user_tz": -120
    },
    "id": "b1676536",
    "outputId": "41f4202b-9e1c-4bf3-b88f-318f3502b0db",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "queries = relevance_jud['preferences'].tolist()\n",
    "similarity_df = pd.DataFrame(similarity_matrix, columns = queries)\n",
    "#similarity_df.insert(loc=0, column = 'doc_id', value=data['cord_uid'].tolist())\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be0b7b9",
   "metadata": {},
   "source": [
    "- relevance judgements matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()\n",
    "data.drop(['index'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a89854",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_rels = np.zeros((data.shape[0], relevance_jud.shape[0]))\n",
    "\n",
    "for n, pref in enumerate(relevance_jud.iterrows()):\n",
    "    gust = pref[1]['preferences'].split(',')\n",
    "    gust = [g[1:] for g in gust]\n",
    "    user_i = data[data['Genre'].isin(gust)]\n",
    "    for i in user_i.index:\n",
    "        users_rels[i,n] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c3f7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "real_values = pd.DataFrame(users_rels, columns=queries).astype(int)\n",
    "real_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eab5647",
   "metadata": {},
   "source": [
    "- Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a4251",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_roc, ax_roc = plt.subplots(figsize=(16, 8))\n",
    "fig_pr, ax_pr = plt.subplots(figsize=(16, 8))\n",
    "for i in range(len(queries)):\n",
    "    print(\"Query \" + str(i) + \": \" + queries[i])\n",
    "    \n",
    "    # Steps:\n",
    "    # 1. Selection of the query\n",
    "    # 2. Selection of the non-negatives\n",
    "    # 3. Merge cosine distances and relevance judgements\n",
    "    r_i = real_values.rename(columns={queries[i]: 'judgement'})\n",
    "    q_i = similarity_df.rename(columns = {queries[i]: 'query'})\n",
    "    r_i = r_i[['judgement']]\n",
    "    q_i = q_i[['query']]\n",
    "    rq_i = pd.merge(q_i, r_i, how='left',left_index=True, right_index=True)\n",
    "    # 4. Sort according to distance\n",
    "    rq_i = rq_i.sort_values(by='query', ascending = False)\n",
    "    # 5. Separate and evaluate\n",
    "    q_i = rq_i[['query']]\n",
    "    r_i = rq_i[['judgement']]\n",
    "    r_i_np = r_i.T.to_numpy()\n",
    "    q_i_np = q_i.T.to_numpy()\n",
    "    \n",
    "    eval = 'all'\n",
    "    x_axis, y_axis = evaluate(eval, q_i_np, r_i_np)\n",
    "\n",
    "         \n",
    "    if eval == 'prec_rec' or eval == 'all':  \n",
    "        ax_pr.plot(x_axis[0], y_axis[0])\n",
    "\n",
    "    if eval == 'auc' or eval == 'roc' or eval == 'all':\n",
    "        if eval == 'all':\n",
    "            ax_roc.plot(x_axis[1], y_axis[1])\n",
    "        else: \n",
    "            ax_roc.plot(x_axis[0], y_axis[0])\n",
    "\n",
    "ax_pr.set_xlabel('Recall')\n",
    "ax_pr.set_ylabel('Precision')\n",
    "ax_pr.legend([f'{queries[i]}' for i in range(len(queries))])\n",
    "fig_pr.show()\n",
    "\n",
    "\n",
    "ax_roc.set_xlabel('FPR')\n",
    "ax_roc.set_ylabel('TPR')\n",
    "ax_roc.legend([f'{queries[i]}' for i in range(len(queries))])\n",
    "fig_roc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a3316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
